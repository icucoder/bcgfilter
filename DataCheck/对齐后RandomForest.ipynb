{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec12f8c0-df4a-48aa-83c5-ca3b2612f5f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m     label[i\u001b[38;5;241m*\u001b[39moneperson_nums:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39moneperson_nums] \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# 数据对齐\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m Unite_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/root/zqh/Save_Model/United_model_device.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     86\u001b[0m feature1, ans, feature2 \u001b[38;5;241m=\u001b[39m Unite_model(data\u001b[38;5;241m.\u001b[39mcuda())\n\u001b[1;32m     87\u001b[0m features \u001b[38;5;241m=\u001b[39m feature2\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 809\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1174\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1142\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:1116\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1112\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[38;5;241m.\u001b[39mUntypedStorage)\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1116\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1117\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1118\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1121\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 217\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:187\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(obj\u001b[38;5;241m.\u001b[39mnbytes(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(location))\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_utils.py:84\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m     )\n\u001b[0;32m---> 84\u001b[0m     \u001b[43muntyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 导入数据集\n",
    "from Toolkit import *\n",
    "import sys  \n",
    "sys.path.append('/root/zqh/filter_test')\n",
    "from FingerPrint_5_quick import *\n",
    "from United_model import *\n",
    "\n",
    "Pathlist = [\n",
    "    '/root/zqh/NewDataSet/BCG_ZQH2.pt',\n",
    "    '/root/zqh/NewDataSet/BCG_QJF1.pt',\n",
    "    '/root/zqh/NewDataSet/BCG_WCM1.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa1.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa2.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa3.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa5.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa6.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa7.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa8.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa9.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa10.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa11.pt', # poor\n",
    "    '/root/zqh/NewDataSet/New_data_pa12.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa13.pt', # poor\n",
    "    '/root/zqh/NewDataSet/New_data_pa14.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa15.pt', # poor\n",
    "    '/root/zqh/NewDataSet/New_data_pa16.pt', # poor\n",
    "    '/root/zqh/NewDataSet/New_data_pa17.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa18.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa19.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa20.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa21.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa22.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa23.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa24.pt', # poor\n",
    "    '/root/zqh/NewDataSet/New_data_pa25.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa26.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa27.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa28.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa29.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa30.pt', # poor\n",
    "    '/root/zqh/NewDataSet/New_data_pa31.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa32.pt', # poor\n",
    "    '/root/zqh/NewDataSet/New_data_pa33.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa34.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa35.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa36.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa37.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa38.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa39.pt',\n",
    "    '/root/zqh/NewDataSet/New_data_pa40.pt',\n",
    "    '/root/zqh/BCGDataSet/modify_extract_Single_resolution_caoan615.pt',\n",
    "    '/root/zqh/BCGDataSet/modify_extract_Single_resolution_dj613.pt',\n",
    "    '/root/zqh/BCGDataSet/modify_extract_Single_resolution_dxt613.pt',\n",
    "    '/root/zqh/BCGDataSet/modify_extract_Single_resolution_ltm613.pt',\n",
    "    '/root/zqh/BCGDataSet/modify_extract_Single_resolution_qjf612.pt',\n",
    "    '/root/zqh/BCGDataSet/modify_extract_Single_resolution_rrx613.pt',\n",
    "    '/root/zqh/BCGDataSet/modify_extract_Single_resolution_sample1.pt',\n",
    "    '/root/zqh/BCGDataSet/modify_extract_Single_resolution_sjj612.pt',\n",
    "    '/root/zqh/BCGDataSet/modify_extract_Single_resolution_tt612.pt',\n",
    "    '/root/zqh/BCGDataSet/modify_extract_Single_resolution_wcm612.pt',\n",
    "    '/root/zqh/BCGDataSet/modify_extract_Single_resolution_wg613.pt',\n",
    "    '/root/zqh/BCGDataSet/modify_extract_Single_resolution_whd612.pt',\n",
    "    '/root/zqh/BCGDataSet/modify_extract_Single_resolution_wxy.pt',\n",
    "    '/root/zqh/BCGDataSet/modify_extract_Single_resolution_zj612.pt',\n",
    "    # '/root/zqh/BCGDataSet/modify_extract_Single_resolution_zqh1.pt',\n",
    "    '/root/zqh/BCGDataSet/modify_extract_Single_resolution_zzp612.pt',\n",
    "]\n",
    "oneperson_begin = 0\n",
    "oneperson_end = 30\n",
    "oneperson_nums = oneperson_end - oneperson_begin\n",
    "persons = len(Pathlist)\n",
    "data = get_ResUnet_data(Pathlist=Pathlist, oneperson_begin=oneperson_begin, oneperson_end=oneperson_end)[:,:,:].detach()\n",
    "label = torch.zeros(oneperson_nums*persons)\n",
    "for i in range(persons):\n",
    "    label[i*oneperson_nums:(i+1)*oneperson_nums] = i\n",
    "\n",
    "# 数据对齐\n",
    "Unite_model = torch.load('/root/zqh/Save_Model/United_model_device.pth').cuda().eval()\n",
    "feature1, ans, feature2 = Unite_model(data.cuda())\n",
    "features = feature2\n",
    "data = features.detach()\n",
    "\n",
    "def splitDataSet(data, label, persons, oneperson_nums): # 要求输入 shape: N x\n",
    "    X_train = data[0:20,:]\n",
    "    X_test = data[20:30,:]\n",
    "    y_train = label[0:20]\n",
    "    y_test = label[20:30]\n",
    "    for i in range(1,persons):\n",
    "        X_train = torch.cat([X_train, data[oneperson_nums*i:oneperson_nums*i+20,:]],dim=0)\n",
    "        X_test = torch.cat([X_test, data[oneperson_nums*i+20:oneperson_nums*i+30,:]],dim=0)\n",
    "        y_train = torch.cat([y_train, label[oneperson_nums*i:oneperson_nums*i+20]],dim=0)\n",
    "        y_test = torch.cat([y_test, label[oneperson_nums*i+20:oneperson_nums*i+30]],dim=0)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = splitDataSet(data.cpu().squeeze(1), label, persons, oneperson_nums)\n",
    "# 导入数据集\n",
    "\n",
    "# 训练随机森林\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "def MyRandomForest(persons, X_train, X_test, y_train, y_test, k):\n",
    "    right = 0\n",
    "    oneperson_nums = 10\n",
    "    for i in range(X_test.shape[0]):\n",
    "        # 切分数据\n",
    "        databegin = i//oneperson_nums*oneperson_nums\n",
    "        if k==1:\n",
    "            Kset = X_test[i:i+1,:]\n",
    "        else:\n",
    "            _,remainer,_,_ = train_test_split(X_test[databegin:databegin+oneperson_nums,:],\n",
    "                                              X_test[databegin:databegin+oneperson_nums,:],\n",
    "                                              test_size=(k-1)/oneperson_nums,\n",
    "                                              random_state=i)\n",
    "            Kset = torch.cat([X_test[i:i+1,:],remainer],dim=0)\n",
    "        # 测试\n",
    "        record = torch.zeros(persons)\n",
    "        predicted_list = model.predict(Kset)\n",
    "        for j in range(k):\n",
    "            record[int(predicted_list[j])] += 1\n",
    "        maxvalue,maxindex = torch.max(record,dim=0)\n",
    "        if maxindex == y_test[i]:\n",
    "            right+=1\n",
    "    return right/y_test.shape[0]\n",
    "\n",
    "# 结果\n",
    "list = []\n",
    "k_range = 9\n",
    "for i in range(1, k_range):\n",
    "    ans = MyRandomForest(persons, X_train, X_test, y_train, y_test, k=i)\n",
    "    list.append(ans)\n",
    "arraylist = np.array(list)\n",
    "print(arraylist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
